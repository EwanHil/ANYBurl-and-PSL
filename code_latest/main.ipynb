{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c83270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment if you need to install pslpython\n",
    "#pip install pslpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 09:49:29 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "\n",
    "from classes.DatasetGenerator import generate_dataset_file\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.PSLFileBuilder import PSLFileBuilder\n",
    "from classes.Dataset import get_dataset\n",
    "from classes.Dataset import get_dataset_name\n",
    "\n",
    "#Setting this to True is required to get all files needed for PSL,\n",
    "#but is very costly\n",
    "CREATE_FILES = True \n",
    "\n",
    "def pre_main():\n",
    "    dataset = get_dataset()\n",
    "    \n",
    "    train_triples = dataset.training.mapped_triples.numpy()\n",
    "    val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    test_triples = dataset.testing.mapped_triples.numpy()   \n",
    "    dataset_name = get_dataset_name()\n",
    "    generate_dataset_file('train.txt',dataset_name,train_triples,dataset)\n",
    "    generate_dataset_file('valid.txt',dataset_name,val_triples,dataset)\n",
    "    generate_dataset_file('test.txt',dataset_name,test_triples,dataset)\n",
    "    \n",
    "    if CREATE_FILES:       \n",
    "        train_triples = dataset.training.mapped_triples.numpy()\n",
    "        val_triples = dataset.validation.mapped_triples.numpy()\n",
    "        #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "    \n",
    "        entity_converter = EntityConverter(dataset)\n",
    "        create_files(train_triples,val_triples,entity_converter)\n",
    "    \n",
    "    #Create files needed by PSL\n",
    "def create_files(train_triples, val_triples,entity_converter):    \n",
    "    filebuilder = PSLFileBuilder(train_triples, val_triples, entity_converter)\n",
    "    filebuilder.build_map_files()\n",
    "    filebuilder.build_obs_files()\n",
    "    #filebuilder.build_target_files()\n",
    "    filebuilder.build_truth_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 08:04:34 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from pslpython.model import Model\n",
    "from pslpython.partition import Partition\n",
    "from pslpython.predicate import Predicate\n",
    "from pslpython.rule import Rule\n",
    "from classes.Dataset import get_dataset\n",
    "from classes.ANYBurlToPSLConverter import ANYBurlToPSLConverter\n",
    "from classes.RuleImporter import RuleImporter\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.DatasetGenerator import encode_text\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from classes.PSLFileBuilder import build_target_files\n",
    "from classes.DatasetGenerator import encode_text\n",
    "from classes.PSLFileBuilder import PSLFileBuilder\n",
    "import glob\n",
    "\n",
    "MODEL_NAME = 'ANYBurl and PSL Model'\n",
    "\n",
    "DATA_DIR = os.path.join('data')\n",
    "\n",
    "ADDITIONAL_PSL_OPTIONS = {\n",
    "    'log4j.threshold': 'INFO'\n",
    "}\n",
    "\n",
    "ADDITIONAL_CLI_OPTIONS = [\n",
    "    # '--postgres'\n",
    "]\n",
    "\n",
    "MAX_RULES = None\n",
    "ANYBURL_RULES_THRESHOLD = 0.6\n",
    "PREDICT_PREDICATES_ONE_AT_A_TIME = False\n",
    "\n",
    "def main():\n",
    "    print(f\"PSL run stated at {get_date_time()}\")\n",
    "    importer = RuleImporter()\n",
    "    importer.import_rules()\n",
    "   \n",
    "    dataset = get_dataset()\n",
    "    entity_converter = EntityConverter(dataset)\n",
    "\n",
    "    train_triples = dataset.training.mapped_triples.numpy()\n",
    "    val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "    \n",
    "    #Delete all pre-existing predicates from the folder\n",
    "    for f in glob.glob(\"inferred-predicates/*\"):\n",
    "                    os.remove(f)\n",
    "            \n",
    "    if PREDICT_PREDICATES_ONE_AT_A_TIME :\n",
    "        for relindex,name in tqdm(entity_converter.relindex_to_name.items()): \n",
    "            try:\n",
    "                model = Model(MODEL_NAME)\n",
    "                \n",
    "                #Delete existing targets files so we only deal with one at a time\n",
    "                for f in glob.glob(\"data/targets/*\"):\n",
    "                    os.remove(f)\n",
    "                \n",
    "                #Add Target Files\n",
    "                build_target_files(name,relindex, entity_converter, train_triples)\n",
    "                # Add Predicates\n",
    "                add_predicates(model,entity_converter,name)\n",
    "\n",
    "                # Add Rules\n",
    "                add_rules(model,importer.rules, encode_text(name), name)\n",
    "\n",
    "                # Add Data\n",
    "                add_data(model, entity_converter, name)\n",
    "\n",
    "                # Inference\n",
    "                results = infer(model)\n",
    "                write_results(results, model)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                continue\n",
    "    else:\n",
    "        name = \"All Predicates\"\n",
    "        model = Model(MODEL_NAME)\n",
    "\n",
    "        #Add Target Files\n",
    "        filebuilder = PSLFileBuilder(train_triples, val_triples, entity_converter)\n",
    "        filebuilder.build_target_files()\n",
    "        # Add Predicates\n",
    "        add_predicates(model,entity_converter)\n",
    "\n",
    "        # Add Rules\n",
    "        add_rules(model,importer.rules, None)\n",
    "\n",
    "        # Add Data\n",
    "        add_data(model, entity_converter)\n",
    "\n",
    "        # Inference\n",
    "        results = infer(model)\n",
    "        write_results(results, model)\n",
    "    print(f\"PSL run ended at {get_date_time()}\")\n",
    "    \n",
    "def add_predicates(model,entity_converter,target_name=None):\n",
    "    if target_name is not None:\n",
    "        print(f\"Adding predicates for {target_name}\")\n",
    "    else:\n",
    "        print(\"Adding predicates for All Predicates\")\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):\n",
    "        is_closed = False\n",
    "        if target_name is not None:\n",
    "            is_closed = name != target_name\n",
    "        predicate = Predicate(encode_text(name), closed = is_closed, size = 2)\n",
    "        model.add_predicate(predicate)\n",
    "\n",
    "def add_rules(model, rules, name, target_name=None):\n",
    "    if target_name is not None:\n",
    "        print(f\"Adding rules for {target_name}\")\n",
    "    else:\n",
    "        print(\"Adding rules for All Predicates\")\n",
    "    converter = ANYBurlToPSLConverter(rules)\n",
    "    total_rules = 0\n",
    "    for rule in tqdm(converter.converted_rules):\n",
    "        if name is None or rule.split('->')[1].split('(')[0].replace(' ','') == name:\n",
    "            if MAX_RULES != None and total_rules >= MAX_RULES:\n",
    "                print(f\"Maximum number of rules added ({total_rules} rules added)\")\n",
    "                return\n",
    "            if float(rule.split(':')[0]) > ANYBURL_RULES_THRESHOLD:\n",
    "                model.add_rule(Rule(rule))\n",
    "                total_rules += 1\n",
    "                continue\n",
    "                \n",
    "    if total_rules < 1:\n",
    "        raise Exception(f\"Failed to add any rules. Aborting inference for {target_name}\")\n",
    "    print(f\"{total_rules} rules added\")\n",
    "    \n",
    "def add_data(model,entity_converter, target_name = None):\n",
    "    if target_name is not None:\n",
    "        print(f\"Adding data for {target_name}\")\n",
    "    else:\n",
    "        print(\"Adding data for All Predicates\")\n",
    "        \n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):        \n",
    "        path = f'data/obs/{encode_text(name)}_obs.txt'\n",
    "        if path_exists(path): #Check file has content before adding  \n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.OBSERVATIONS, path)\n",
    "\n",
    "        path =  f'data/targets/{encode_text(name)}_targets.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TARGETS, path)\n",
    "\n",
    "        path = f'data/truth/{encode_text(name)}_truth.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TRUTH, path)\n",
    "   \n",
    "def path_exists(path):\n",
    "    try:\n",
    "        return os.path.getsize(f\"{path}\") > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def infer(model): \n",
    "    print(f\"Inference starting at {get_date_time()}\")\n",
    "    return model.infer(psl_config = ADDITIONAL_PSL_OPTIONS)\n",
    "     \n",
    "def get_date_time():\n",
    "    return f\"{str(datetime.now().time()).split('.')[0]} on {datetime.today().strftime('%d-%b-%Y')}\"\n",
    "\n",
    "def write_results(results, model):\n",
    "    print(f\"Inferenced completed at {get_date_time()}\")\n",
    "    out_dir = 'inferred-predicates'\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    print(\"Writing predicates\")\n",
    "    for predicate in tqdm(model.get_predicates().values()):\n",
    "        if (predicate.closed()):\n",
    "            continue       \n",
    "        try:\n",
    "            out_path = os.path.join(out_dir, \"%s.txt\" % (predicate.name()))\n",
    "            results[predicate].to_csv(out_path, sep = \"\\t\", header = False, index = False)     \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fa046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8545aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shared Python 3.8 (default)",
   "language": "python",
   "name": "sharedpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
