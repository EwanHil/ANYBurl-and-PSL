{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8666aa42-8310-4487-96da-63c7df91c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pslpython in ./.local/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: pyyaml>=3.13 in /opt/conda/envs/sharedpy38/lib/python3.8/site-packages (from pslpython) (6.0)\n",
      "Requirement already satisfied: pandas>=0.24.1 in ./.local/lib/python3.8/site-packages (from pslpython) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.local/lib/python3.8/site-packages (from pandas>=0.24.1->pslpython) (1.23.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas>=0.24.1->pslpython) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas>=0.24.1->pslpython) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sharedpy38/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=0.24.1->pslpython) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pslpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61aab3c4-dad2-4528-ba26-1e0eb1d5cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sharedpy38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/sharedpy38/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 09:49:29 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "\n",
    "from classes.DatasetGenerator import generate_dataset_file\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.PSLFileBuilder import PSLFileBuilder\n",
    "from pykeen.datasets import CoDExSmall\n",
    "\n",
    "#Setting this to True is required to get all files needed for PSL,\n",
    "#but is very costly\n",
    "CREATE_FILES = True \n",
    "\n",
    "def pre_main():\n",
    "    dataset = CoDExSmall()\n",
    "    \n",
    "    train_triples = dataset.training.mapped_triples.numpy()\n",
    "    val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    test_triples = dataset.testing.mapped_triples.numpy()   \n",
    "    \n",
    "    generate_dataset_file('train.txt','CoDEx',train_triples,dataset)\n",
    "    generate_dataset_file('valid.txt','CoDEx',val_triples,dataset)\n",
    "    generate_dataset_file('test.txt','CoDEx',test_triples,dataset)\n",
    "    \n",
    "    if CREATE_FILES:\n",
    "        dataset = CoDExSmall()\n",
    "        \n",
    "        train_triples = dataset.training.mapped_triples.numpy()\n",
    "        val_triples = dataset.validation.mapped_triples.numpy()\n",
    "        #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "    \n",
    "        entity_converter = EntityConverter(dataset)\n",
    "        create_files(train_triples,val_triples,entity_converter)\n",
    "    \n",
    "    #Create files needed by PSL\n",
    "def create_files(train_triples, val_triples,entity_converter):    \n",
    "    filebuilder = PSLFileBuilder(train_triples, val_triples, entity_converter)\n",
    "    filebuilder.build_map_files()\n",
    "    filebuilder.build_obs_files()\n",
    "    filebuilder.build_target_files()\n",
    "    filebuilder.build_truth_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8d9dc2-7c73-4ae4-ab53-124a77cf149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 08:04:34 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from pslpython.model import Model\n",
    "from pslpython.partition import Partition\n",
    "from pslpython.predicate import Predicate\n",
    "from pslpython.rule import Rule\n",
    "from pykeen.datasets import CoDExSmall\n",
    "from classes.ANYBurlToPSLConverter import ANYBurlToPSLConverter\n",
    "from classes.RuleImporter import RuleImporter\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.DatasetGenerator import encode_text\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "MODEL_NAME = 'ANYBurl and PSL Model'\n",
    "\n",
    "DATA_DIR = os.path.join('data')\n",
    "\n",
    "ADDITIONAL_PSL_OPTIONS = {\n",
    "    'log4j.threshold': 'INFO'\n",
    "}\n",
    "\n",
    "ADDITIONAL_CLI_OPTIONS = [\n",
    "    # '--postgres'\n",
    "]\n",
    "\n",
    "MAX_RULES = 1\n",
    "ANYBURL_RULES_THRESHOLD = 0.6\n",
    "\n",
    "def main():\n",
    "    importer = RuleImporter()\n",
    "    importer.import_rules()\n",
    "\n",
    "    model = Model(MODEL_NAME)\n",
    "    dataset = CoDExSmall()\n",
    "    entity_converter = EntityConverter(dataset)\n",
    "\n",
    "    #train_triples = dataset.training.mapped_triples.numpy()\n",
    "    #val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "\n",
    "    # Add Predicates\n",
    "    add_predicates(model,entity_converter)\n",
    "\n",
    "    # Add Rules\n",
    "    add_rules(model,importer.rules)\n",
    "\n",
    "    # Inference\n",
    "    results = infer(model,entity_converter)\n",
    "    write_results(results, model)\n",
    " \n",
    "def add_predicates(model,entity_converter): \n",
    "    print(\"Adding predicates...\")\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):\n",
    "        predicate = Predicate(encode_text(name), closed = name != 'genre', size = 2)\n",
    "        model.add_predicate(predicate)\n",
    "\n",
    "def add_rules(model, rules):\n",
    "    print(\"Adding rules...\")\n",
    "    converter = ANYBurlToPSLConverter(rules)\n",
    "    total_rules = 0\n",
    "    for rule in tqdm(converter.converted_rules):\n",
    "        if rule.split('->')[1].split('(')[0].replace(' ','') == 'genre':\n",
    "            if total_rules >= MAX_RULES:\n",
    "                print(f\"Maximum number of rules added ({total_rules} rules added)\")\n",
    "                return\n",
    "            if float(rule.split(':')[0]) > ANYBURL_RULES_THRESHOLD:\n",
    "                model.add_rule(Rule(rule))\n",
    "                total_rules += 1\n",
    "                continue\n",
    "    print(f\"{total_rules} rules added\")\n",
    "    \n",
    "def add_data(model,entity_converter):\n",
    "    print(\"Adding data...\")\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):        \n",
    "        path = f'data/obs/{encode_text(name)}_obs.txt'\n",
    "        if path_exists(path): #Check file has content before adding  \n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.OBSERVATIONS, path)\n",
    "\n",
    "        path =  f'data/targets/{encode_text(name)}_targets.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TARGETS, path)\n",
    "\n",
    "        path = f'data/truth/{encode_text(name)}_truth.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TRUTH, path)\n",
    "   \n",
    "def path_exists(path):\n",
    "    try:\n",
    "        return os.path.getsize(f\"{path}\") > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def infer(model,entity_converter):\n",
    "    add_data(model, entity_converter)\n",
    "    print(f\"Inference starting at {get_date_time()}\")\n",
    "    return model.infer(additional_cli_options = ADDITIONAL_CLI_OPTIONS, psl_config = ADDITIONAL_PSL_OPTIONS)\n",
    "     \n",
    "def get_date_time():\n",
    "    return f\"{str(datetime.now().time()).split('.')[0]} on {datetime.today().strftime('%d-%b-%Y')}\"\n",
    "\n",
    "def write_results(results, model):\n",
    "    print(f\"Inferenced completed at {get_date_time()}\")\n",
    "    out_dir = 'inferred-predicates'\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    print(\"Writing predicates\")\n",
    "    for predicate in tqdm(model.get_predicates().values()):\n",
    "        if (predicate.closed()):\n",
    "            continue       \n",
    "        try:\n",
    "            out_path = os.path.join(out_dir, \"%s.txt\" % (predicate.name()))\n",
    "            results[predicate].to_csv(out_path, sep = \"\\t\", header = False, index = False)     \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4c55b3-417b-43ce-96e5-53fc0dfc7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CoDEx train.txt dataset file in destination: datasets/data/CoDEx/train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32888/32888 [00:00<00:00, 526195.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CoDEx valid.txt dataset file in destination: datasets/data/CoDEx/valid.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1827/1827 [00:00<00:00, 490714.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CoDEx test.txt dataset file in destination: datasets/data/CoDEx/test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1828/1828 [00:00<00:00, 497191.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building _map.txt files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building _obs.txt files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 36.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building _targets.txt files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 33.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building _truth.txt files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 646.23it/s]\n"
     ]
    }
   ],
   "source": [
    "pre_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db92f08-6cff-4b9a-907b-4b2db945787b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding predicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 676.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding rules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1125/27266 [00:00<00:00, 1301680.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of rules added (1 rules added)\n",
      "Adding data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:11<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference starting at 14:24:38 on 29-Aug-2022\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea899624-b753-4929-9273-195501d9dbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shared Python 3.8 (default)",
   "language": "python",
   "name": "sharedpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
