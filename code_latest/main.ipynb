{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666aa42-8310-4487-96da-63c7df91c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pslpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aab3c4-dad2-4528-ba26-1e0eb1d5cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 09:49:29 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "\n",
    "from classes.DatasetGenerator import generate_dataset_file\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.PSLFileBuilder import PSLFileBuilder\n",
    "from pykeen.datasets import CoDExSmall\n",
    "\n",
    "#Setting this to True is required to get all files needed for PSL,\n",
    "#but is very costly\n",
    "CREATE_FILES = True \n",
    "\n",
    "def pre_main():\n",
    "    dataset = CoDExSmall()\n",
    "    \n",
    "    train_triples = dataset.training.mapped_triples.numpy()\n",
    "    val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    test_triples = dataset.testing.mapped_triples.numpy()   \n",
    "    \n",
    "    generate_dataset_file('train.txt','CoDEx',train_triples,dataset)\n",
    "    generate_dataset_file('valid.txt','CoDEx',val_triples,dataset)\n",
    "    generate_dataset_file('test.txt','CoDEx',test_triples,dataset)\n",
    "    \n",
    "    if CREATE_FILES:\n",
    "        dataset = CoDExSmall()\n",
    "        \n",
    "        train_triples = dataset.training.mapped_triples.numpy()\n",
    "        val_triples = dataset.validation.mapped_triples.numpy()\n",
    "        #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "    \n",
    "        entity_converter = EntityConverter(dataset)\n",
    "        create_files(train_triples,val_triples,entity_converter)\n",
    "    \n",
    "    #Create files needed by PSL\n",
    "def create_files(train_triples, val_triples,entity_converter):    \n",
    "    filebuilder = PSLFileBuilder(train_triples, val_triples, entity_converter)\n",
    "    filebuilder.build_map_files()\n",
    "    filebuilder.build_obs_files()\n",
    "    #filebuilder.build_target_files()\n",
    "    filebuilder.build_truth_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d9dc2-7c73-4ae4-ab53-124a77cf149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  4 08:04:34 2022\n",
    "\n",
    "@author: ewanhilton\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from pslpython.model import Model\n",
    "from pslpython.partition import Partition\n",
    "from pslpython.predicate import Predicate\n",
    "from pslpython.rule import Rule\n",
    "from pykeen.datasets import CoDExSmall\n",
    "from classes.ANYBurlToPSLConverter import ANYBurlToPSLConverter\n",
    "from classes.RuleImporter import RuleImporter\n",
    "from classes.EntityConverter import EntityConverter\n",
    "from classes.DatasetGenerator import encode_text\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from classes.PSLFileBuilder import build_target_files\n",
    "from classes.DatasetGenerator import encode_text\n",
    "import glob\n",
    "\n",
    "MODEL_NAME = 'ANYBurl and PSL Model'\n",
    "\n",
    "DATA_DIR = os.path.join('data')\n",
    "\n",
    "ADDITIONAL_PSL_OPTIONS = {\n",
    "    'log4j.threshold': 'INFO'\n",
    "}\n",
    "\n",
    "ADDITIONAL_CLI_OPTIONS = [\n",
    "    # '--postgres'\n",
    "]\n",
    "\n",
    "MAX_RULES = None\n",
    "ANYBURL_RULES_THRESHOLD = 0.6\n",
    "\n",
    "def main():\n",
    "    importer = RuleImporter()\n",
    "    importer.import_rules()\n",
    "   \n",
    "    dataset = CoDExSmall()\n",
    "    entity_converter = EntityConverter(dataset)\n",
    "\n",
    "    train_triples = dataset.training.mapped_triples.numpy()\n",
    "    #val_triples = dataset.validation.mapped_triples.numpy()\n",
    "    #test_triples = dataset.testing.mapped_triples.numpy()  \n",
    "\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()): \n",
    "        try:\n",
    "            for f in glob.glob(\"data/targets/*\"):\n",
    "                os.remove(f)\n",
    "            model = Model(MODEL_NAME)\n",
    "            build_target_files(name,relindex, entity_converter, train_triples)\n",
    "            # Add Predicates\n",
    "            add_predicates(model,entity_converter,name)\n",
    "\n",
    "            # Add Rules\n",
    "            add_rules(model,importer.rules, encode_text(name), name)\n",
    "\n",
    "            # Add Data\n",
    "            add_data(model, entity_converter, name)\n",
    "\n",
    "            # Inference\n",
    "            results = infer(model)\n",
    "            write_results(results, model)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            continue\n",
    " \n",
    "def add_predicates(model,entity_converter,target_name): \n",
    "    print(f\"Adding predicates for {target_name}\")\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):\n",
    "        predicate = Predicate(encode_text(name), closed = name != target_name, size = 2)\n",
    "        model.add_predicate(predicate)\n",
    "\n",
    "def add_rules(model, rules, name, target_name):\n",
    "    print(f\"Adding rules for {target_name}\")\n",
    "    converter = ANYBurlToPSLConverter(rules)\n",
    "    total_rules = 0\n",
    "    for rule in tqdm(converter.converted_rules):\n",
    "        if rule.split('->')[1].split('(')[0].replace(' ','') == name:\n",
    "            if MAX_RULES != None and total_rules >= MAX_RULES:\n",
    "                print(f\"Maximum number of rules added ({total_rules} rules added)\")\n",
    "                return\n",
    "            if float(rule.split(':')[0]) > ANYBURL_RULES_THRESHOLD:\n",
    "                model.add_rule(Rule(rule))\n",
    "                total_rules += 1\n",
    "                continue\n",
    "                \n",
    "    if total_rules < 1:\n",
    "        raise Exception(f\"Failed to add any rules. Aborting inference for {target_name}\")\n",
    "    print(f\"{total_rules} rules added\")\n",
    "    \n",
    "def add_data(model,entity_converter, target_name):\n",
    "    print(f\"Adding data for {target_name}\")\n",
    "    for relindex,name in tqdm(entity_converter.relindex_to_name.items()):        \n",
    "        path = f'data/obs/{encode_text(name)}_obs.txt'\n",
    "        if path_exists(path): #Check file has content before adding  \n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.OBSERVATIONS, path)\n",
    "\n",
    "        path =  f'data/targets/{encode_text(name)}_targets.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TARGETS, path)\n",
    "\n",
    "        path = f'data/truth/{encode_text(name)}_truth.txt'\n",
    "        if path_exists(path):\n",
    "            model.get_predicate(encode_text(name)).add_data_file(Partition.TRUTH, path)\n",
    "   \n",
    "def path_exists(path):\n",
    "    try:\n",
    "        return os.path.getsize(f\"{path}\") > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def infer(model): \n",
    "    print(f\"Inference starting at {get_date_time()}\")\n",
    "    return model.infer(additional_cli_options = ADDITIONAL_CLI_OPTIONS, psl_config = ADDITIONAL_PSL_OPTIONS)\n",
    "     \n",
    "def get_date_time():\n",
    "    return f\"{str(datetime.now().time()).split('.')[0]} on {datetime.today().strftime('%d-%b-%Y')}\"\n",
    "\n",
    "def write_results(results, model):\n",
    "    print(f\"Inferenced completed at {get_date_time()}\")\n",
    "    out_dir = 'inferred-predicates'\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    print(\"Writing predicates\")\n",
    "    for predicate in tqdm(model.get_predicates().values()):\n",
    "        if (predicate.closed()):\n",
    "            continue       \n",
    "        try:\n",
    "            out_path = os.path.join(out_dir, \"%s.txt\" % (predicate.name()))\n",
    "            results[predicate].to_csv(out_path, sep = \"\\t\", header = False, index = False)     \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c55b3-417b-43ce-96e5-53fc0dfc7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db92f08-6cff-4b9a-907b-4b2db945787b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65875bd-8494-4abd-bfdf-16d3ef3a8b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32871469-7f96-42ae-8e4f-3892c3d4671a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shared Python 3.8 (default)",
   "language": "python",
   "name": "sharedpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
